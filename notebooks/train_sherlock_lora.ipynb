{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f797bdb7-03e6-45ca-9e63-d680d9d3f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft bitsandbytes\n",
    "!pip install -q -U trl  \n",
    "!pip install -q -U sentencepiece  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24176b6-8bce-4624-a039-a7a7c5ff13e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "패키지 버전 확인\n",
      "================================================================================\n",
      "transformers: 4.57.1\n",
      "datasets: 4.4.1\n",
      "peft: 0.18.0\n",
      "torch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA RTX 4000 Ada Generation\n",
      "GPU Memory: 19.7 GB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"패키지 버전 확인\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"datasets: {datasets.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e325e2f1-67b5-41c9-be2c-976b545d4d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "데이터셋 로딩\n",
      "================================================================================\n",
      "데이터셋 로드 완료\n",
      "총 샘플 수: 140\n",
      "\n",
      "데이터셋 구조:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'story_id', 'story_title'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"데이터셋 로딩\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "quiz_dataset = load_dataset(\"Alleinzellgaenger/sherlock-holmes-qa\")\n",
    "\n",
    "print(f\"데이터셋 로드 완료\")\n",
    "print(f\"총 샘플 수: {len(quiz_dataset['train'])}\")\n",
    "print(\"\\n데이터셋 구조:\")\n",
    "print(quiz_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0893765-9979-4f66-b19b-4b45e2c536b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "데이터 샘플 확인 (처음 3개)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "샘플 1\n",
      "================================================================================\n",
      "출처: I. A SCANDAL IN BOHEMIA\n",
      "질문: How does Holmes regard emotions and love, and in what way is Irene Adler an exception to his usual s...\n",
      "정답 길이: 379 글자\n",
      "\n",
      "================================================================================\n",
      "샘플 2\n",
      "================================================================================\n",
      "출처: II. THE RED-HEADED LEAGUE\n",
      "질문: How did the Red-Headed League scheme function, and what was its ultimate purpose?...\n",
      "정답 길이: 389 글자\n",
      "\n",
      "================================================================================\n",
      "샘플 3\n",
      "================================================================================\n",
      "출처: III. A CASE OF IDENTITY\n",
      "질문: Which concrete clues and observations led Holmes to conclude that \"Mr. Hosmer Angel\" was actually Mi...\n",
      "정답 길이: 480 글자\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"데이터 샘플 확인 (처음 3개)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    sample = quiz_dataset['train'][i]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"샘플 {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"출처: {sample['story_title']}\")\n",
    "    print(f\"질문: {sample['question'][:100]}...\")\n",
    "    print(f\"정답 길이: {len(sample['answer'])} 글자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7857869-4057-4c00-88e2-ba810cbbf4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "데이터 형식 변환 (Gemma instruction format)\n",
      "================================================================================\n",
      "데이터 변환 완료: 140개\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"데이터 형식 변환 (Gemma instruction format)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def format_for_gemma(example):\n",
    "    \"\"\"\n",
    "    Gemma-2 instruction format으로 변환\n",
    "    \"\"\"\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "{example['question']}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{example['answer']}<end_of_turn>\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# 데이터 변환\n",
    "formatted_dataset = quiz_dataset['train'].map(format_for_gemma)\n",
    "\n",
    "print(f\"데이터 변환 완료: {len(formatted_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda4b875-5003-496b-92e6-c687a9b488ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "변환된 데이터 샘플 (처음 500자):\n",
      "<start_of_turn>user\n",
      "How does Holmes regard emotions and love, and in what way is Irene Adler an exception to his usual stance?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Holmes regards emotions—especially love—as abhorrent intrusions that would disturb his perfectly balanced reasoning, likening them to grit in a sensitive instrument; nevertheless Irene Adler is uniquely remarkable to him—“the” woman who eclipses all others in his eyes, though he claims no romantic attachment and remembers her only as bei\n",
      "...\n",
      "\n",
      "Train: 126개\n",
      "Val: 14개\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 변환된 샘플 확인\n",
    "print(\"\\n변환된 데이터 샘플 (처음 500자):\")\n",
    "print(formatted_dataset[0]['text'][:500])\n",
    "print(\"...\\n\")\n",
    "\n",
    "# Train/Val 분할\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train: {len(split_dataset['train'])}개\")\n",
    "print(f\"Val: {len(split_dataset['test'])}개\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1fae06-3b3d-469d-a527-1b374989e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메모리 완전 정리\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# 기존 객체 삭제\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"메모리 완전 정리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5552faaf-6933-405e-8d10-a8e63bbef593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "모델 로딩\n",
      "================================================================================\n",
      "토크나이저 로딩...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 로드 완료\n",
      "\n",
      "모델 로딩 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc09178e84124127a54f742d61c61bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료!\n",
      "\n",
      "총 파라미터: 2,614,341,888 (2.61B)\n",
      "\n",
      "GPU 메모리: 4.87 GB / 4.95 GB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"모델 로딩\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# 토크나이저\n",
    "print(\"토크나이저 로딩...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"토크나이저 로드 완료\")\n",
    "\n",
    "# 모델\n",
    "print(\"\\n모델 로딩 중...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"모델 로드 완료!\")\n",
    "\n",
    "# 파라미터 수\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n총 파라미터: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "\n",
    "# GPU 메모리\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\nGPU 메모리: {allocated:.2f} GB / {reserved:.2f} GB\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffc04f5-7738-429c-bcde-fdf90eb492d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "데이터 토크나이징\n",
      "================================================================================\n",
      "토크나이징 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e5b4e4b173406f81daa93df345ac09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이징 완료\n",
      "  - Train: 126개\n",
      "  - Val: 14개\n",
      "\n",
      "토큰 길이: 512\n",
      "디코딩 샘플: <bos><start_of_turn>user\n",
      "How did Holmes structure his investigation using “two lines” of inquiry, and why was that distinction important to solving the case?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Holmes split the problem into line A (the domestic scandal around Lady Beatrice — moods...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"데이터 토크나이징\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"토크나이징 함수\"\"\"\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# 토크나이징 실행\n",
    "print(\"토크나이징 중...\")\n",
    "tokenized_train = split_dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_dataset['train'].column_names\n",
    ")\n",
    "\n",
    "tokenized_val = split_dataset['test'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_dataset['test'].column_names\n",
    ")\n",
    "\n",
    "print(f\"토크나이징 완료\")\n",
    "print(f\"  - Train: {len(tokenized_train)}개\")\n",
    "print(f\"  - Val: {len(tokenized_val)}개\")\n",
    "\n",
    "# 샘플 확인\n",
    "sample = tokenized_train[0]\n",
    "print(f\"\\n토큰 길이: {len(sample['input_ids'])}\")\n",
    "print(f\"디코딩 샘플: {tokenizer.decode(sample['input_ids'][:50])}...\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f0a79dd-8ff3-44a4-8859-0475c16d1b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LoRA 설정 및 적용\n",
      "================================================================================\n",
      "LoRA 설정:\n",
      "  - rank: 16\n",
      "  - alpha: 32\n",
      "  - target_modules: {'k_proj', 'q_proj', 'v_proj', 'o_proj'}\n",
      "\n",
      "LoRA 적용 완료!\n",
      "trainable params: 6,389,760 || all params: 2,620,731,648 || trainable%: 0.2438\n",
      "\n",
      "학습 파라미터: 6,389,760 (6.39M)\n",
      "학습 비율: 0.2438%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LoRA 설정 및 적용\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"LoRA 설정:\")\n",
    "print(f\"  - rank: {lora_config.r}\")\n",
    "print(f\"  - alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  - target_modules: {lora_config.target_modules}\")\n",
    "\n",
    "# LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nLoRA 적용 완료!\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# gradient 활성화\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n학습 파라미터: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "print(f\"학습 비율: {100 * trainable_params / total_params:.4f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6483ad3e-e0d1-48ab-931e-29f8942b85d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Arguments 설정\n",
      "================================================================================\n",
      "Training Arguments 설정 완료\n",
      "\n",
      "주요 설정:\n",
      "  - Epochs: 10\n",
      "  - Batch size: 2\n",
      "  - Gradient accumulation: 4\n",
      "  - Effective batch size: 8\n",
      "  - Learning rate: 0.0002\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training Arguments 설정\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/sherlock-lora\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,  # LoRA는 좀 더 높은 LR\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"../outputs/logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "print(\"Training Arguments 설정 완료\")\n",
    "print(f\"\\n주요 설정:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de251f7-8a79-424e-b8f9-aa908396fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Trainer 초기화\n",
      "================================================================================\n",
      "Trainer 초기화 완료!\n",
      "\n",
      "학습 준비:\n",
      "  - 모델: Gemma-2-2B + LoRA (r=16)\n",
      "  - Train: 126개\n",
      "  - Val: 14개\n",
      "  - Early Stopping: patience=2\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Trainer 초기화\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=default_data_collator,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Trainer 초기화 완료!\")\n",
    "print(f\"\\n학습 준비:\")\n",
    "print(f\"  - 모델: Gemma-2-2B + LoRA (r=16)\")\n",
    "print(f\"  - Train: {len(tokenized_train)}개\")\n",
    "print(f\"  - Val: {len(tokenized_val)}개\")\n",
    "print(f\"  - Early Stopping: patience=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db30c3d-f7f1-4ae0-b506-d40b814223c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "학습 시작!\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 68/160 02:05 < 02:54, 0.53 it/s, Epoch 4.19/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.475400</td>\n",
       "      <td>0.853224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.719100</td>\n",
       "      <td>0.760740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.653500</td>\n",
       "      <td>0.738366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.741240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 메모리 최종 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"학습 시작!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 학습 실행\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"학습 완료!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10ad12a7-0ceb-48a9-a0e9-33d07f984e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 저장 중...\n",
      "모델 저장 완료: ./sherlock-lora-final\n",
      "\n",
      "================================================================================\n",
      "모든 작업 완료!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n모델 저장 중...\")\n",
    "model.save_pretrained(\"../models/sherlock-lora-final\")\n",
    "tokenizer.save_pretrained(\"../models/sherlock-lora-final\")\n",
    "print(\"모델 저장 완료: ../models/sherlock-lora-final\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"모든 작업 완료!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345bfb4-f396-4dc2-a665-546c6e4d9a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
